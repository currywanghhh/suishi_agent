import os
import json
import time
import mysql.connector
import requests
from django.shortcuts import render
from django.http import StreamingHttpResponse
from dotenv import load_dotenv

load_dotenv()

# Configuration
SILICON_FLOW_API_URL = 'https://api.siliconflow.cn/v1/chat/completions'
SILICON_FLOW_API_KEY = os.getenv('SILICON_FLOW_API_KEY')

# å»æ‰ç¯å¢ƒå˜é‡ä¸­å¯èƒ½çš„å¼•å·
if SILICON_FLOW_API_KEY:
    SILICON_FLOW_API_KEY = SILICON_FLOW_API_KEY.strip('"').strip("'")

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost').strip('"').strip("'"),
    'user': os.getenv('DB_USER', 'root').strip('"').strip("'"),
    'password': os.getenv('DB_PASSWORD', '').strip('"').strip("'"),
    'database': os.getenv('DB_NAME', 'mysql').strip('"').strip("'"),
    'pool_name': 'mypool',
    'pool_size': 5,
    'pool_reset_session': True,
    'autocommit': True
}

# ä½¿ç”¨å¿«é€Ÿæ¨¡å‹è¿›è¡Œç®€å•çš„ ID é€‰æ‹©ä»»åŠ¡
LLM_MODEL = "Qwen/Qwen3-32B"  # ä¿®æ­£æ¨¡å‹åç§°
# å¸¸è§çš„å¯ç”¨æ¨¡å‹ï¼š
# - "Qwen/Qwen2.5-7B-Instruct"
# - "deepseek-ai/DeepSeek-V2.5"
# - "01-ai/Yi-1.5-9B-Chat"

# ä¼šè¯ç®¡ç†ï¼šå­˜å‚¨å¤šè½®å¯¹è¯å†å²ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis/æ•°æ®åº“ï¼‰
SESSION_STORE = {}
# ç»“æ„: {session_id: {'history': [{'role': 'user', 'content': '...'}, ...], 'l4_id': int, 'l4_content': dict}}

def get_or_create_session(session_id):
    """è·å–æˆ–åˆ›å»ºä¼šè¯"""
    if session_id not in SESSION_STORE:
        SESSION_STORE[session_id] = {
            'history': [],
            'l4_id': None,
            'l4_content': None
        }
    return SESSION_STORE[session_id]

def add_to_history(session_id, role, content):
    """æ·»åŠ æ¶ˆæ¯åˆ°ä¼šè¯å†å²"""
    session = get_or_create_session(session_id)
    session['history'].append({'role': role, 'content': content})
    # é™åˆ¶å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘10è½®ï¼‰
    if len(session['history']) > 20:  # 10è½®å¯¹è¯ = 20æ¡æ¶ˆæ¯
        session['history'] = session['history'][-20:]


def index(request):
    """Render the main advisor interface"""
    return render(request, 'advisor/index.html')

# ========== ç®€åŒ–ç‰ˆé…ç½®ï¼ˆç§»é™¤å¤æ‚çš„äººæ ¼æ˜ å°„ï¼‰ ==========
# ç›´æ¥ã€ç®€å•çš„å†³ç­–é¡¾é—® - ä¸éœ€è¦å¤æ‚çš„äººæ ¼åˆ‡æ¢

def build_contextualized_prompt(user_query, l4_info, conversation_history):
    """æ„å»ºåŸºäºäº”è¡Œç†è®ºçš„ç›´æ¥å†³ç­– prompt"""
    
    # ç³»ç»Ÿè§’è‰²ï¼šäº”è¡Œå†³ç­–é¡¾é—®
    system_role = """You are a Wu Xing (Five Elements) decision advisor who gives direct, confident answers.

Five Elements Principles (use the CONCEPTS, not the labels):
- Wood: Growth, boldness, forward motion â†’ describe as "moving forward", "taking initiative", "expanding"
- Fire: Passion, visibility, expression â†’ describe as "showing up", "being magnetic", "expressing yourself"
- Earth: Stability, grounding, centering â†’ describe as "grounded", "stable", "rooted", "calm"
- Metal: Clarity, structure, boundaries â†’ describe as "clear", "structured", "focused", "decisive"
- Water: Flow, adaptability, intuition â†’ describe as "flowing", "adaptive", "flexible", "intuitive"

Your approach:
1. Diagnose the situation using Five Elements principles (internally)
2. Give ONE clear directive 
3. Explain why using the QUALITIES (grounded, flowing, clear) NOT the element names
4. Keep it under 80 words total

Style rules:
- Say "Do this" NOT "You could try..."
- DON'T say "water energy" or "earth energy" - say "you're scattered" or "you need to be grounded"
- Be conversational and natural - like a wise friend, not a textbook
- Weave in the wisdom naturally, don't lecture about elements

Example:
"Wear beige or brown. You're feeling scattered right now, and those tones will ground youâ€”think of it like hitting pause on the chaos. Add one gold piece for a clean, focused accent. You're not trying to impress; you're showing up centered."
"""
    
    # è¯é¢˜èŒƒå›´å’Œäº”è¡ŒèƒŒæ™¯
    topic_context = f"""
Topic: {l4_info['l4_name']}
Context: {l4_info['l1_name']} â†’ {l4_info['l2_name']} â†’ {l4_info['l3_name']}

Apply Five Elements wisdom to give guidance. Use the qualities naturally in your language.
"""

    # å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    history_text = ""
    if conversation_history:
        recent_history = conversation_history[-20:]  # æœ€è¿‘10è½®
        history_text = "\nPrevious conversation:\n"
        for msg in recent_history:
            role_label = "User" if msg['role'] == 'user' else "You"
            history_text += f"{role_label}: {msg['content']}\n"
    
    # å½“å‰é—®é¢˜
    current_question = f"""
User question: "{user_query}"

Give your direct answer now (under 80 words). Be natural and conversational:"""
    
    # ç»„åˆå®Œæ•´ prompt
    full_prompt = system_role + topic_context + history_text + current_question
    
    return full_prompt


def build_general_prompt(user_query, conversation_history):
    """æ„å»ºé€šç”¨äº”è¡Œ prompt - å½“æ²¡æœ‰åŒ¹é…åˆ°çŸ¥è¯†åº“æ—¶ä½¿ç”¨"""
    
    # ç³»ç»Ÿè§’è‰²ï¼šäº”è¡Œå†³ç­–é¡¾é—®ï¼ˆé€šç”¨ç‰ˆï¼‰
    system_role = """You are a Wu Xing (Five Elements) decision advisor who gives direct, confident answers.

Five Elements Principles (use the CONCEPTS, not the labels):
- Wood: Growth, boldness, forward motion â†’ describe as "moving forward", "taking initiative", "expanding"
- Fire: Passion, visibility, expression â†’ describe as "showing up", "being magnetic", "expressing yourself"
- Earth: Stability, grounding, centering â†’ describe as "grounded", "stable", "rooted", "calm"
- Metal: Clarity, structure, boundaries â†’ describe as "clear", "structured", "focused", "decisive"
- Water: Flow, adaptability, intuition â†’ describe as "flowing", "adaptive", "flexible", "intuitive"

Your approach:
1. Diagnose the situation using Five Elements principles (internally)
2. Give ONE clear directive 
3. Explain why using the QUALITIES (grounded, flowing, clear) NOT the element names
4. Keep it under 80 words total

Style rules:
- Say "Do this" NOT "You could try..."
- DON'T say "water energy" or "earth energy" - say "you're scattered" or "you need to be grounded"
- Be conversational and natural - like a wise friend, not a textbook
- Weave in the wisdom naturally, don't lecture about elements

Example:
"Wear beige or brown. You're feeling scattered right now, and those tones will ground youâ€”think of it like hitting pause on the chaos. Add one gold piece for a clean, focused accent. You're not trying to impress; you're showing up centered."
"""

    # å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    history_text = ""
    if conversation_history:
        recent_history = conversation_history[-20:]  # æœ€è¿‘10è½®
        history_text = "\nPrevious conversation:\n"
        for msg in recent_history:
            role_label = "User" if msg['role'] == 'user' else "You"
            history_text += f"{role_label}: {msg['content']}\n"
    
    # å½“å‰é—®é¢˜
    current_question = f"""
User question: "{user_query}"

Give your direct answer now (under 80 words). Be natural and conversational:"""
    
    # ç»„åˆå®Œæ•´ prompt
    full_prompt = system_role + history_text + current_question
    
    return full_prompt

def generate_decision_header(user_query, l4_info):
    """
    ç”Ÿæˆå†³ç­–å¤´éƒ¨ï¼šä¿¡å·ç¯ + èƒ½é‡ç±»å‹ + æ ¸å¿ƒæŒ‡ä»¤
    ä½¿ç”¨å¿«é€Ÿ LLM è°ƒç”¨ï¼ˆéæµå¼ï¼‰
    """
    if not SILICON_FLOW_API_KEY:
        return None
    
    prompt = f"""Based on this question: "{user_query}"
Topic: {l4_info['l4_name']}

Generate a quick decision header in JSON format:
{{
  "signal": "ğŸŸ¢" or "ğŸŸ¡" or "ğŸ”´",
  "vibe": "one of: Growth Energy / Passion Energy / Grounding Energy / Clarity Energy / Flow Energy",
  "instruction": "one short imperative sentence (5-8 words)"
}}

Rules:
- ğŸŸ¢ Green = Go for it, confident move
- ğŸŸ¡ Yellow = Proceed with caution
- ğŸ”´ Red = Stop, reconsider
- Choose the energy that fits best
- Instruction must be direct and actionable

Respond ONLY with valid JSON, no explanation."""

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {SILICON_FLOW_API_KEY}"
    }
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 150,
        "temperature": 0.5
    }
    
    try:
        response = requests.post(SILICON_FLOW_API_URL, headers=headers, 
                                data=json.dumps(payload), timeout=30)
        result = response.json()
        content = result['choices'][0]['message']['content'].strip()
        
        # å°è¯•è§£æ JSON
        import re
        json_match = re.search(r'\{[^}]+\}', content, re.DOTALL)
        if json_match:
            decision = json.loads(json_match.group())
            return decision
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        return {
            "signal": "ğŸŸ¢",
            "vibe": "Clarity Energy",
            "instruction": "Trust your instinct and move forward"
        }
    except Exception as e:
        print(f"[ERROR] ç”Ÿæˆå†³ç­–å¤´éƒ¨å¤±è´¥: {e}")
        return None

def call_llm_stream(prompt: str):
    """
    Call LLM API with streaming enabled.
    Yields chunks of text as they arrive.
    """
    if not SILICON_FLOW_API_KEY:
        yield "data: Error: API key not configured\n\n"
        return

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {SILICON_FLOW_API_KEY}"
    }
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 2048,
        "temperature": 0.7,
        "stream": True  # Enable streaming
    }

    try:
        response = requests.post(
            SILICON_FLOW_API_URL, 
            headers=headers, 
            data=json.dumps(payload), 
            stream=True,
            timeout=120
        )
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                line_text = line.decode('utf-8')
                if line_text.startswith('data: '):
                    line_text = line_text[6:]  # Remove 'data: ' prefix
                    
                    if line_text.strip() == '[DONE]':
                        break
                        
                    try:
                        data = json.loads(line_text)
                        if 'choices' in data and len(data['choices']) > 0:
                            delta = data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                # Send as SSE format
                                yield f"data: {json.dumps({'content': content})}\n\n"
                    except json.JSONDecodeError:
                        continue
                        
    except Exception as e:
        yield f"data: {json.dumps({'error': str(e)})}\n\n"


def find_best_l4_match(user_query):
    """Find the best matching L4 intention for the user query with hierarchical search"""
    conn = None
    try:
        print(f"[MATCH] å¼€å§‹åŒ¹é…æµç¨‹ï¼Œç”¨æˆ·é—®é¢˜: '{user_query}'")
        
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # Step 1: Find best matching L1 Domain
        print("[MATCH] æ­¥éª¤ 1/4: æŸ¥è¯¢æ‰€æœ‰ L1 é¢†åŸŸ...")
        cursor.execute("SELECT id, name, description_en FROM knowledge_base WHERE level = 1")
        l1_candidates = cursor.fetchall()
        
        print(f"[MATCH] æ‰¾åˆ° {len(l1_candidates)} ä¸ª L1 é¢†åŸŸ")
        
        if not l1_candidates:
            print("[ERROR] æ•°æ®åº“ä¸­æ²¡æœ‰ L1 æ•°æ®ï¼")
            return None
        
        # Use LLM to select best L1
        l1_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:100] if c[2] else ''}" for c in l1_candidates])
        l1_prompt = f"""User Query: "{user_query}"

Available Life Domains (L1):
{l1_list}

Task: Select the single most relevant Life Domain ID that best matches the user's question.
Return ONLY the ID number."""
        
        print("[MATCH] è°ƒç”¨ LLM é€‰æ‹© L1...")
        best_l1_id = call_llm_for_selection(l1_prompt)
        if not best_l1_id:
            print("[ERROR] L1 åŒ¹é…å¤±è´¥ï¼ŒLLM æœªè¿”å›æœ‰æ•ˆ ID")
            return None
        
        print(f"[Match] L1 Domain ID: {best_l1_id}")
        
        # Step 2: Find best matching L2 Scenario under the selected L1
        cursor.execute("""
            SELECT id, name, description_en 
            FROM knowledge_base 
            WHERE level = 2 AND parent_id = %s
        """, (best_l1_id,))
        l2_candidates = cursor.fetchall()
        
        if not l2_candidates:
            return None
        
        l2_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:100] if c[2] else ''}" for c in l2_candidates])
        l2_prompt = f"""User Query: "{user_query}"

Available Scenarios (L2):
{l2_list}

Task: Select the single most relevant Scenario ID that best matches the user's specific situation.
Return ONLY the ID number."""
        
        best_l2_id = call_llm_for_selection(l2_prompt)
        if not best_l2_id:
            return None
        
        print(f"[Match] L2 Scenario ID: {best_l2_id}")
        
        # Step 3: Find best matching L3 Sub-scenario under the selected L2
        cursor.execute("""
            SELECT id, name, description_en 
            FROM knowledge_base 
            WHERE level = 3 AND parent_id = %s
        """, (best_l2_id,))
        l3_candidates = cursor.fetchall()
        
        if not l3_candidates:
            return None
        
        l3_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:80] if c[2] else ''}" for c in l3_candidates])
        l3_prompt = f"""User Query: "{user_query}"

Available Sub-scenarios (L3):
{l3_list}

Task: Select the single most relevant Sub-scenario ID.
Return ONLY the ID number."""
        
        best_l3_id = call_llm_for_selection(l3_prompt)
        if not best_l3_id:
            return None
        
        print(f"[Match] L3 Sub-scenario ID: {best_l3_id}")
        
        # Step 4: Find best matching L4 Intention under the selected L3
        cursor.execute("""
            SELECT kb.id, kb.name, kb.description_en 
            FROM knowledge_base kb
            JOIN l4_content c ON kb.id = c.l4_id
            WHERE kb.level = 4 AND kb.parent_id = %s
        """, (best_l3_id,))
        l4_candidates = cursor.fetchall()
        
        if not l4_candidates:
            # Fallback: try to find any L4 with content under this L3
            cursor.execute("""
                SELECT kb.id, kb.name, kb.description_en 
                FROM knowledge_base kb
                WHERE kb.level = 4 AND kb.parent_id = %s
                LIMIT 1
            """, (best_l3_id,))
            fallback = cursor.fetchone()
            if fallback:
                print(f"[Match] L4 Intention ID (fallback): {fallback[0]}")
                return fallback[0]
            return None
        
        l4_list = "\n".join([f"ID {c[0]}: {c[1]}" for c in l4_candidates])
        l4_prompt = f"""User Query: "{user_query}"

Available User Intentions (L4):
{l4_list}

Task: Select the single most relevant Intention ID that exactly matches what the user wants to know.
Return ONLY the ID number."""
        
        best_l4_id = call_llm_for_selection(l4_prompt)
        print(f"[Match] L4 Intention ID: {best_l4_id}")
        
        return best_l4_id
        
    except Exception as e:
        print(f"Error in find_best_l4_match: {e}")
        return None
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()


def call_llm_for_selection(prompt):
    """Helper function to call LLM and extract ID from response"""
    if not SILICON_FLOW_API_KEY:
        print("[ERROR] API Key æœªé…ç½®ï¼")
        return None
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {SILICON_FLOW_API_KEY}"
    }
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 50,
        "temperature": 0.3
    }
    
    try:
        print(f"[LLM] è°ƒç”¨æ¨¡å‹: {LLM_MODEL}")
        print(f"[LLM] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        response = requests.post(SILICON_FLOW_API_URL, headers=headers, 
                                data=json.dumps(payload), timeout=60)
        
        print(f"[LLM] å“åº”çŠ¶æ€ç : {response.status_code}")
        
        result = response.json()
        
        # å¦‚æœæ˜¯é”™è¯¯å“åº”ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        if response.status_code != 200:
            print(f"[ERROR] API è¿”å›é”™è¯¯: {result}")
            return None
        
        content = result['choices'][0]['message']['content'].strip()
        
        print(f"[LLM] è¿”å›å†…å®¹: '{content}'")
        
        import re
        match = re.search(r'\d+', content)
        if match:
            selected_id = int(match.group())
            print(f"[LLM] æå–çš„ ID: {selected_id}")
            return selected_id
        else:
            print(f"[ERROR] æ— æ³•ä»è¿”å›å†…å®¹ä¸­æå–æ•°å­— ID")
            return None
            
    except Exception as e:
        print(f"[ERROR] LLM è°ƒç”¨å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    
    return None


def get_l4_info(l4_id):
    """Retrieve basic info for a specific L4 ID from knowledge_base"""
    conn = None
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT l4.name, l3.name, l2.name, l1.name
            FROM knowledge_base l4
            JOIN knowledge_base l3 ON l4.parent_id = l3.id
            JOIN knowledge_base l2 ON l3.parent_id = l2.id
            JOIN knowledge_base l1 ON l2.parent_id = l1.id
            WHERE l4.id = %s AND l4.level = 4
        """, (l4_id,))
        
        result = cursor.fetchone()
        if result:
            return {
                'l4_name': result[0],
                'l3_name': result[1],
                'l2_name': result[2],
                'l1_name': result[3]
            }
        return None
        
    except Exception as e:
        print(f"Error in get_l4_info: {e}")
        return None
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()


def generate_stream_response(user_query, session_id='default'):
    """Generate streaming response with L4 knowledge boundary and conversation context"""
    
    import sys
    print(f"\n{'='*60}", flush=True)
    print(f"[STREAM] å¼€å§‹ç”Ÿæˆæµå¼å“åº”", flush=True)
    print(f"[STREAM] Session ID: '{session_id}'", flush=True)
    print(f"[STREAM] ç”¨æˆ·é—®é¢˜: '{user_query}'", flush=True)
    print(f"{'='*60}\n", flush=True)
    sys.stdout.flush()
    
    # è·å–ä¼šè¯
    session = get_or_create_session(session_id)
    
    # Send initial status
    yield f"data: {json.dumps({'status': 'Analyzing your question...'})}\n\n"
    
    # æ¯è½®å¯¹è¯éƒ½é‡æ–°åŒ¹é… L4ï¼Œç¡®ä¿ç²¾å‡†å“åº”
    print("[STREAM] è°ƒç”¨ find_best_l4_match...", flush=True)
    sys.stdout.flush()
    
    l4_id = find_best_l4_match(user_query)
    
    print(f"[STREAM] è¿”å›çš„ L4 ID: {l4_id}", flush=True)
    sys.stdout.flush()
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    add_to_history(session_id, 'user', user_query)
    
    # === å…œåº•é€»è¾‘ï¼šå¦‚æœæ²¡æœ‰åŒ¹é…åˆ° L4ï¼Œç›´æ¥ç”¨é€šç”¨ prompt ===
    if not l4_id:
        print("[STREAM] L4 åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æ¨¡å¼å›ç­”", flush=True)
        sys.stdout.flush()
        
        # å‘é€çŠ¶æ€æç¤º
        yield f"data: {json.dumps({'status': 'Answering your question...'})}\n\n"
        
        # æ„å»ºé€šç”¨ promptï¼ˆä¸ä¾èµ–çŸ¥è¯†åº“ï¼‰
        prompt = build_general_prompt(user_query, session['history'][:-1])
        
        print(f"[STREAM] ä½¿ç”¨é€šç”¨ Promptï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦", flush=True)
        
        # è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
        assistant_response = ""
        for chunk in call_llm_stream(prompt):
            if chunk.startswith("data:"):
                yield chunk
                try:
                    data = json.loads(chunk[6:])
                    if 'content' in data:
                        assistant_response += data['content']
                except:
                    pass
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        if assistant_response:
            add_to_history(session_id, 'assistant', assistant_response)
        
        # å‘é€å®Œæˆä¿¡å·
        yield "data: [DONE]\n\n"
        print("[STREAM] æµå¼å“åº”å®Œæˆï¼ˆé€šç”¨æ¨¡å¼ï¼‰", flush=True)
        sys.stdout.flush()
        return
    
    # === æ­£å¸¸æµç¨‹ï¼šåŒ¹é…åˆ°äº† L4 ===
    # Get L4 basic info as semantic boundary
    l4_info = get_l4_info(l4_id)
    
    if not l4_info:
        print("[STREAM] L4ä¿¡æ¯è·å–å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æ¨¡å¼å›ç­”", flush=True)
        sys.stdout.flush()
        
        # å‘é€çŠ¶æ€æç¤º
        yield f"data: {json.dumps({'status': 'Answering your question...'})}\n\n"
        
        # æ„å»ºé€šç”¨ prompt
        prompt = build_general_prompt(user_query, session['history'][:-1])
        
        print(f"[STREAM] ä½¿ç”¨é€šç”¨ Promptï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦", flush=True)
        
        # è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
        assistant_response = ""
        for chunk in call_llm_stream(prompt):
            if chunk.startswith("data:"):
                yield chunk
                try:
                    data = json.loads(chunk[6:])
                    if 'content' in data:
                        assistant_response += data['content']
                except:
                    pass
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        if assistant_response:
            add_to_history(session_id, 'assistant', assistant_response)
        
        # å‘é€å®Œæˆä¿¡å·
        yield "data: [DONE]\n\n"
        print("[STREAM] æµå¼å“åº”å®Œæˆï¼ˆé€šç”¨æ¨¡å¼ - L4ä¿¡æ¯ç¼ºå¤±ï¼‰", flush=True)
        sys.stdout.flush()
        return
    
    # æ›´æ–°ä¼šè¯ä¸­çš„ L4 ä¿¡æ¯
    session['l4_id'] = l4_id
    session['l4_info'] = l4_info
    
    # Send matched topic
    topic_name = l4_info['l4_name']
    matched_msg = {'status': f'Topic: {topic_name}', 'section': 'header'}
    yield f"data: {json.dumps(matched_msg)}\n\n"
    
    # æ„å»º promptï¼ˆç®€æ´ç‰ˆï¼‰
    prompt = build_contextualized_prompt(user_query, l4_info, session['history'][:-1])  # å†å²ä¸åŒ…å«å½“å‰é—®é¢˜
    
    print(f"[STREAM] æ„å»ºçš„ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦", flush=True)
    
    # è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
    assistant_response = ""
    for chunk in call_llm_stream(prompt):
        if chunk.startswith("data:"):
            yield chunk
            # æå–å†…å®¹ç´¯ç§¯ï¼ˆç”¨äºä¿å­˜åˆ°å†å²ï¼‰
            try:
                data = json.loads(chunk[6:])
                if 'content' in data:
                    assistant_response += data['content']
            except:
                pass
    
    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
    if assistant_response:
        add_to_history(session_id, 'assistant', assistant_response)
    
    # Send completion
    yield "data: [DONE]\n\n"
    print(f"[STREAM] æµå¼å“åº”å®Œæˆ", flush=True)
    sys.stdout.flush()


def ask_advisor(request):
    """Handle streaming responses for user questions"""
    if request.method == 'POST':
        user_query = request.POST.get('query', '').strip()
        # ä»è¯·æ±‚ä¸­è·å–æˆ–ç”Ÿæˆ session_id
        session_id = request.POST.get('session_id', '').strip()
        if not session_id:
            import uuid
            session_id = str(uuid.uuid4())
            print(f"[SESSION] ç”Ÿæˆæ–°ä¼šè¯ID: {session_id}")
        else:
            print(f"[SESSION] ä½¿ç”¨ç°æœ‰ä¼šè¯ID: {session_id}")
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"\n{'='*60}")
        print(f"[REQUEST] æ”¶åˆ°ç”¨æˆ·é—®é¢˜: '{user_query}'")
        print(f"[REQUEST] ä¼šè¯ID: {session_id}")
        print(f"[REQUEST] API Keyå­˜åœ¨: {bool(SILICON_FLOW_API_KEY)}")
        print(f"[REQUEST] ä½¿ç”¨æ¨¡å‹: {LLM_MODEL}")
        print(f"{'='*60}\n")
        
        if not user_query:
            print("[ERROR] ç”¨æˆ·é—®é¢˜ä¸ºç©º")
            return StreamingHttpResponse(
                iter([f"data: {json.dumps({'error': 'Please enter a question'})}\n\n"]),
                content_type='text/event-stream'
            )
        
        response = StreamingHttpResponse(
            generate_stream_response(user_query, session_id),
            content_type='text/event-stream'
        )
        response['Cache-Control'] = 'no-cache'
        response['X-Session-ID'] = session_id  # é€šè¿‡å“åº”å¤´è¿”å› session_id
        response['X-Accel-Buffering'] = 'no'
        return response
    
    return render(request, 'advisor/index.html')

