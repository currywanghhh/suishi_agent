import os
import json
import time
import mysql.connector
import requests
from django.shortcuts import render
from django.http import StreamingHttpResponse
from dotenv import load_dotenv

load_dotenv()

# Configuration
# LLM Provider: 'silicon_flow' or 'ollama'
LLM_PROVIDER = os.getenv('LLM_PROVIDER', 'ollama').strip('"').strip("'").lower()

# Silicon Flow Configuration
SILICON_FLOW_API_URL = 'https://api.siliconflow.cn/v1/chat/completions'
SILICON_FLOW_API_KEY = os.getenv('SILICON_FLOW_API_KEY')
if SILICON_FLOW_API_KEY:
    SILICON_FLOW_API_KEY = SILICON_FLOW_API_KEY.strip('"').strip("'")

# Ollama Configuration
OLLAMA_API_URL = os.getenv('OLLAMA_API_URL', 'http://localhost:11434/api/chat').strip('"').strip("'")
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'gemma3:4b').strip('"').strip("'")

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost').strip('"').strip("'"),
    'user': os.getenv('DB_USER', 'root').strip('"').strip("'"),
    'password': os.getenv('DB_PASSWORD', '').strip('"').strip("'"),
    'database': os.getenv('DB_NAME', 'mysql').strip('"').strip("'"),
    'pool_name': 'mypool',
    'pool_size': 5,
    'pool_reset_session': True,
    'autocommit': True
}

# LLM Model (æ ¹æ®providerè‡ªåŠ¨é€‰æ‹©)
if LLM_PROVIDER == 'ollama':
    LLM_MODEL = OLLAMA_MODEL
    LLM_API_URL = OLLAMA_API_URL
    LLM_API_KEY = None  # Ollamaä¸éœ€è¦API Key
else:
    LLM_MODEL = "Qwen/Qwen3-32B"
    LLM_API_URL = SILICON_FLOW_API_URL
    LLM_API_KEY = SILICON_FLOW_API_KEY

# ä¼šè¯ç®¡ç†ï¼šå­˜å‚¨å¤šè½®å¯¹è¯å†å²ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis/æ•°æ®åº“ï¼‰
SESSION_STORE = {}
# ç»“æ„: {session_id: {'history': [{'role': 'user', 'content': '...'}, ...], 'l4_id': int, 'l4_content': dict}}

def get_or_create_session(session_id):
    """è·å–æˆ–åˆ›å»ºä¼šè¯"""
    if session_id not in SESSION_STORE:
        SESSION_STORE[session_id] = {
            'history': [],
            'l4_id': None,
            'l4_content': None
        }
    return SESSION_STORE[session_id]

def add_to_history(session_id, role, content):
    """æ·»åŠ æ¶ˆæ¯åˆ°ä¼šè¯å†å²"""
    session = get_or_create_session(session_id)
    session['history'].append({'role': role, 'content': content})
    # é™åˆ¶å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘10è½®ï¼‰
    if len(session['history']) > 20:  # 10è½®å¯¹è¯ = 20æ¡æ¶ˆæ¯
        session['history'] = session['history'][-20:]


def index(request):
    """Render the main advisor interface"""
    return render(request, 'advisor/index.html')

# ========== ç®€åŒ–ç‰ˆé…ç½®ï¼ˆç§»é™¤å¤æ‚çš„äººæ ¼æ˜ å°„ï¼‰ ==========
# ç›´æ¥ã€ç®€å•çš„å†³ç­–é¡¾é—® - ä¸éœ€è¦å¤æ‚çš„äººæ ¼åˆ‡æ¢

# ========== V4 æ–°å¢ï¼šæ–‡åŒ–æ˜ å°„è¡¨åŠ è½½ ==========
def load_cultural_mapping():
    """åŠ è½½50å·æ–‡åŒ–æ˜ å°„è¡¨"""
    try:
        mapping_path = os.path.join(os.path.dirname(__file__), 'cultural_mapping.json')
        with open(mapping_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"[Cultural] Failed to load mapping: {e}")
        return None

def get_cultural_context(state_name):
    """æ ¹æ®ç”¨æˆ·æ‰€åœ¨å·è·å–æ–‡åŒ–ä¸Šä¸‹æ–‡æç¤º"""
    mapping = load_cultural_mapping()
    if not mapping:
        return ""
    
    if state_name and state_name in mapping.get('states', {}):
        state_info = mapping['states'][state_name]
        return f"""
=== Cultural Context - FOR YOUR REFERENCE ONLY ===
{state_info['prompt_text']}

IMPORTANT: Use this cultural background to INFORM your advice style, but:
- DO NOT mention the state name (e.g., "Since you're from Kentucky...")
- DO NOT explicitly reference their location
- Instead, naturally adapt your tone, examples, and suggestions to resonate with their background
- Say things like "given your values" or "based on what matters to you" if needed
===
"""
    return ""

def build_contextualized_prompt(user_query, l4_info, conversation_history, bazi_text=None, user_state=None):
    """æ„å»ºåŸºäºäº”è¡Œç†è®ºçš„ç›´æ¥å†³ç­– prompt"""
    
    # ç³»ç»Ÿè§’è‰²ï¼šäº”è¡Œå†³ç­–é¡¾é—®
    system_role = """You are a Wu Xing (Five Elements) personal growth advisor who empowers users to become their strongest, best selves.

Core Mission:
Every piece of advice you give should help the user GROW STRONGER, make BETTER DECISIONS, and become a MORE CAPABLE person. Frame your guidance as tools for self-improvement and personal mastery.

Five Elements Principles (use the CONCEPTS, not the labels):
- Wood: Growth, boldness, forward motion - describe as "moving forward", "taking initiative", "expanding your potential"
- Fire: Passion, visibility, expression - describe as "stepping into your power", "being magnetic", "expressing your authentic self"
- Earth: Stability, grounding, centering - describe as "building your foundation", "staying grounded", "cultivating inner strength"
- Metal: Clarity, structure, boundaries - describe as "sharpening your focus", "setting clear boundaries", "making decisive moves"
- Water: Flow, adaptability, intuition - describe as "trusting your instincts", "adapting with wisdom", "flowing through challenges"

Your approach:
1. Diagnose the situation using Five Elements principles (internally)
2. Give ONE clear directive that makes them STRONGER
3. Explain how this action builds their capability or character
4. Keep it under 80 words total

Style rules:
- Say "Do this" NOT "You could try..." - be confident and empowering
- DON'T say "water energy" or "earth energy" - say "you're building strength" or "you're developing clarity"
- Be like a wise coach who believes in their potential
- Every answer should leave them feeling MORE capable, not dependent

Example:
"Wear beige or brown. Right now you're scattered - these grounded tones will help you center your power. Add one gold piece for sharp focus. You're not trying to impress anyone; you're showing up as someone who knows their own strength."
"""
    
    # === V2 æ–°å¢ï¼šå…«å­—ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰ ===
    bazi_context = ""
    if bazi_text:
        bazi_context = f"""
=== User's Bazi (Birth Chart) - FOR YOUR REFERENCE ONLY ===
{bazi_text}

IMPORTANT: Use this Bazi information as BACKGROUND CONTEXT to inform your advice, but:
- DO NOT mention specific Bazi terms like "å·±äº¥", "ç”²æœ¨", "å¤§è¿" etc. in your response
- DO NOT say "based on your Bazi" or "your birth chart shows"
- Instead, say things like "based on your natural tendencies" or "given your strengths"
- Weave the insights naturally without revealing the source
===
"""
    
    # === V4 æ–°å¢ï¼šæ–‡åŒ–ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰ ===
    cultural_context = get_cultural_context(user_state)
    
    # è¯é¢˜èŒƒå›´å’Œäº”è¡ŒèƒŒæ™¯
    topic_context = f"""
Topic: {l4_info['l4_name']}
Context: {l4_info['l1_name']} > {l4_info['l2_name']} > {l4_info['l3_name']}

Apply Five Elements wisdom to give guidance. Use the qualities naturally in your language.
"""

    # å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    history_text = ""
    if conversation_history:
        recent_history = conversation_history[-20:]  # æœ€è¿‘10è½®
        history_text = "\nPrevious conversation:\n"
        for msg in recent_history:
            role_label = "User" if msg['role'] == 'user' else "You"
            history_text += f"{role_label}: {msg['content']}\n"
    
    # å½“å‰é—®é¢˜
    current_question = f"""
User question: "{user_query}"

Give your direct answer now (under 80 words). Be natural and conversational:"""
    
    # ç»„åˆå®Œæ•´ prompt
    full_prompt = system_role + bazi_context + cultural_context + topic_context + history_text + current_question
    
    return full_prompt


def build_general_prompt(user_query, conversation_history, bazi_text=None, user_state=None):
    """æ„å»ºé€šç”¨äº”è¡Œ prompt - å½“æ²¡æœ‰åŒ¹é…åˆ°çŸ¥è¯†åº“æ—¶ä½¿ç”¨"""
    
    # ç³»ç»Ÿè§’è‰²ï¼šäº”è¡Œå†³ç­–é¡¾é—®ï¼ˆé€šç”¨ç‰ˆï¼‰
    system_role = """You are a Wu Xing (Five Elements) personal growth advisor who empowers users to become their strongest, best selves.

Core Mission:
Every piece of advice you give should help the user GROW STRONGER, make BETTER DECISIONS, and become a MORE CAPABLE person. Frame your guidance as tools for self-improvement and personal mastery.

Five Elements Principles (use the CONCEPTS, not the labels):
- Wood: Growth, boldness, forward motion - describe as "moving forward", "taking initiative", "expanding your potential"
- Fire: Passion, visibility, expression - describe as "stepping into your power", "being magnetic", "expressing your authentic self"
- Earth: Stability, grounding, centering - describe as "building your foundation", "staying grounded", "cultivating inner strength"
- Metal: Clarity, structure, boundaries - describe as "sharpening your focus", "setting clear boundaries", "making decisive moves"
- Water: Flow, adaptability, intuition - describe as "trusting your instincts", "adapting with wisdom", "flowing through challenges"

Your approach:
1. Diagnose the situation using Five Elements principles (internally)
2. Give ONE clear directive that makes them STRONGER
3. Explain how this action builds their capability or character
4. Keep it under 80 words total

Style rules:
- Say "Do this" NOT "You could try..." - be confident and empowering
- DON'T say "water energy" or "earth energy" - say "you're building strength" or "you're developing clarity"
- Be like a wise coach who believes in their potential
- Every answer should leave them feeling MORE capable, not dependent

Example:
"Wear beige or brown. Right now you're scattered - these grounded tones will help you center your power. Add one gold piece for sharp focus. You're not trying to impress anyone; you're showing up as someone who knows their own strength."
"""

    # === V2 æ–°å¢ï¼šå…«å­—ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰ ===
    bazi_context = ""
    if bazi_text:
        bazi_context = f"""
=== User's Bazi (Birth Chart) - FOR YOUR REFERENCE ONLY ===
{bazi_text}

IMPORTANT: Use this Bazi information as BACKGROUND CONTEXT to inform your advice, but:
- DO NOT mention specific Bazi terms like "å·±äº¥", "ç”²æœ¨", "å¤§è¿" etc. in your response
- DO NOT say "based on your Bazi" or "your birth chart shows"
- Instead, say things like "based on your natural tendencies" or "given your strengths"
- Weave the insights naturally without revealing the source
===
"""

    # === V4 æ–°å¢ï¼šæ–‡åŒ–ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰ ===
    cultural_context = get_cultural_context(user_state)

    # å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
    history_text = ""
    if conversation_history:
        recent_history = conversation_history[-20:]  # æœ€è¿‘10è½®
        history_text = "\nPrevious conversation:\n"
        for msg in recent_history:
            role_label = "User" if msg['role'] == 'user' else "You"
            history_text += f"{role_label}: {msg['content']}\n"
    
    # å½“å‰é—®é¢˜
    current_question = f"""
User question: "{user_query}"

Give your direct answer now (under 80 words). Be natural and conversational:"""
    
    # ç»„åˆå®Œæ•´ prompt
    full_prompt = system_role + bazi_context + cultural_context + history_text + current_question
    
    return full_prompt

def generate_decision_header(user_query, l4_info):
    """
    ç”Ÿæˆå†³ç­–å¤´éƒ¨ï¼šä¿¡å·ç¯ + èƒ½é‡ç±»å‹ + æ ¸å¿ƒæŒ‡ä»¤
    ä½¿ç”¨å¿«é€Ÿ LLM è°ƒç”¨ï¼ˆéæµå¼ï¼‰
    """
    if LLM_PROVIDER == 'silicon_flow' and not SILICON_FLOW_API_KEY:
        return None
    
    prompt = f"""Based on this question: "{user_query}"
Topic: {l4_info['l4_name']}

Generate a quick decision header in JSON format:
{{
  "signal": "ğŸŸ¢" or "ğŸŸ¡" or "ğŸ”´",
  "vibe": "one of: Growth Energy / Passion Energy / Grounding Energy / Clarity Energy / Flow Energy",
  "instruction": "one short imperative sentence (5-8 words)"
}}

Rules:
- ğŸŸ¢ Green = Go for it, confident move
- ğŸŸ¡ Yellow = Proceed with caution
- ğŸ”´ Red = Stop, reconsider
- Choose the energy that fits best
- Instruction must be direct and actionable

Respond ONLY with valid JSON, no explanation."""

    headers = {"Content-Type": "application/json"}
    if LLM_API_KEY:
        headers["Authorization"] = f"Bearer {LLM_API_KEY}"
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 150,
        "temperature": 0.5
    }
    
    try:
        response = requests.post(LLM_API_URL, headers=headers, 
                                data=json.dumps(payload), timeout=30)
        result = response.json()
        content = result['choices'][0]['message']['content'].strip() if 'choices' in result else result.get('message', {}).get('content', '').strip()
        
        # å°è¯•è§£æ JSON
        import re
        json_match = re.search(r'\{[^}]+\}', content, re.DOTALL)
        if json_match:
            decision = json.loads(json_match.group())
            return decision
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        return {
            "signal": "ğŸŸ¢",
            "vibe": "Clarity Energy",
            "instruction": "Trust your instinct and move forward"
        }
    except Exception as e:
        print(f"[ERROR] ç”Ÿæˆå†³ç­–å¤´éƒ¨å¤±è´¥: {e}")
        return None

def call_llm_stream(prompt: str):
    """
    Call LLM API with streaming enabled.
    Yields chunks of text as they arrive.
    """
    if LLM_PROVIDER == 'silicon_flow' and not SILICON_FLOW_API_KEY:
        yield "data: Error: API key not configured\n\n"
        return

    headers = {"Content-Type": "application/json"}
    if LLM_API_KEY:
        headers["Authorization"] = f"Bearer {LLM_API_KEY}"
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "stream": True
    }
    
    # Silicon Flowæ ¼å¼éœ€è¦max_tokens
    if LLM_PROVIDER == 'silicon_flow':
        payload["max_tokens"] = 2048
        payload["temperature"] = 0.7

    try:
        response = requests.post(
            LLM_API_URL, 
            headers=headers, 
            data=json.dumps(payload), 
            stream=True,
            timeout=120
        )
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                line_text = line.decode('utf-8')
                
                # Silicon Flowæ ¼å¼
                if line_text.startswith('data: '):
                    line_text = line_text[6:]
                    if line_text.strip() == '[DONE]':
                        break
                    try:
                        data = json.loads(line_text)
                        if 'choices' in data and len(data['choices']) > 0:
                            delta = data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                yield f"data: {json.dumps({'content': content})}\n\n"
                    except json.JSONDecodeError:
                        continue
                # Ollamaæ ¼å¼ï¼ˆç›´æ¥è¿”å›JSONï¼‰
                else:
                    try:
                        data = json.loads(line_text)
                        if 'message' in data:
                            content = data['message'].get('content', '')
                            if content:
                                yield f"data: {json.dumps({'content': content})}\n\n"
                        if data.get('done', False):
                            break
                    except json.JSONDecodeError:
                        continue
                        
    except Exception as e:
        yield f"data: {json.dumps({'error': str(e)})}\n\n"


def find_best_l4_match(user_query):
    """Find the best matching L4 intention for the user query with hierarchical search"""
    conn = None
    try:
        print(f"[MATCH] å¼€å§‹åŒ¹é…æµç¨‹ï¼Œç”¨æˆ·é—®é¢˜: '{user_query}'")
        
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # Step 1: Find best matching L1 Domain
        print("[MATCH] æ­¥éª¤ 1/4: æŸ¥è¯¢æ‰€æœ‰ L1 é¢†åŸŸ...")
        cursor.execute("SELECT id, name, description_en FROM knowledge_base WHERE level = 1")
        l1_candidates = cursor.fetchall()
        
        print(f"[MATCH] æ‰¾åˆ° {len(l1_candidates)} ä¸ª L1 é¢†åŸŸ")
        
        if not l1_candidates:
            print("[ERROR] æ•°æ®åº“ä¸­æ²¡æœ‰ L1 æ•°æ®ï¼")
            return None
        
        # Use LLM to select best L1
        l1_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:100] if c[2] else ''}" for c in l1_candidates])
        l1_prompt = f"""User Query: "{user_query}"

Available Life Domains (L1):
{l1_list}

Task: Select the single most relevant Life Domain ID that best matches the user's question.
Return ONLY the ID number."""
        
        print("[MATCH] è°ƒç”¨ LLM é€‰æ‹© L1...")
        best_l1_id = call_llm_for_selection(l1_prompt)
        if not best_l1_id:
            print("[ERROR] L1 åŒ¹é…å¤±è´¥ï¼ŒLLM æœªè¿”å›æœ‰æ•ˆ ID")
            return None
        
        print(f"[Match] L1 Domain ID: {best_l1_id}")
        
        # Step 2: Find best matching L2 Scenario under the selected L1
        cursor.execute("""
            SELECT id, name, description_en 
            FROM knowledge_base 
            WHERE level = 2 AND parent_id = %s
        """, (best_l1_id,))
        l2_candidates = cursor.fetchall()
        
        if not l2_candidates:
            return None
        
        l2_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:100] if c[2] else ''}" for c in l2_candidates])
        l2_prompt = f"""User Query: "{user_query}"

Available Scenarios (L2):
{l2_list}

Task: Select the single most relevant Scenario ID that best matches the user's specific situation.
Return ONLY the ID number."""
        
        best_l2_id = call_llm_for_selection(l2_prompt)
        if not best_l2_id:
            return None
        
        print(f"[Match] L2 Scenario ID: {best_l2_id}")
        
        # Step 3: Find best matching L3 Sub-scenario under the selected L2
        cursor.execute("""
            SELECT id, name, description_en 
            FROM knowledge_base 
            WHERE level = 3 AND parent_id = %s
        """, (best_l2_id,))
        l3_candidates = cursor.fetchall()
        
        if not l3_candidates:
            return None
        
        l3_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:80] if c[2] else ''}" for c in l3_candidates])
        l3_prompt = f"""User Query: "{user_query}"

Available Sub-scenarios (L3):
{l3_list}

Task: Select the single most relevant Sub-scenario ID.
Return ONLY the ID number."""
        
        best_l3_id = call_llm_for_selection(l3_prompt)
        if not best_l3_id:
            return None
        
        print(f"[Match] L3 Sub-scenario ID: {best_l3_id}")
        
        # Step 4: Find best matching L4 Intention under the selected L3
        cursor.execute("""
            SELECT kb.id, kb.name, kb.description_en 
            FROM knowledge_base kb
            JOIN l4_content c ON kb.id = c.l4_id
            WHERE kb.level = 4 AND kb.parent_id = %s
        """, (best_l3_id,))
        l4_candidates = cursor.fetchall()
        
        if not l4_candidates:
            # Fallback: try to find any L4 with content under this L3
            cursor.execute("""
                SELECT kb.id, kb.name, kb.description_en 
                FROM knowledge_base kb
                WHERE kb.level = 4 AND kb.parent_id = %s
                LIMIT 1
            """, (best_l3_id,))
            fallback = cursor.fetchone()
            if fallback:
                print(f"[Match] L4 Intention ID (fallback): {fallback[0]}")
                return fallback[0]
            return None
        
        l4_list = "\n".join([f"ID {c[0]}: {c[1]}" for c in l4_candidates])
        l4_prompt = f"""User Query: "{user_query}"

Available User Intentions (L4):
{l4_list}

Task: Select the single most relevant Intention ID that exactly matches what the user wants to know.
Return ONLY the ID number."""
        
        best_l4_id = call_llm_for_selection(l4_prompt)
        print(f"[Match] L4 Intention ID: {best_l4_id}")
        
        return best_l4_id
        
    except Exception as e:
        print(f"Error in find_best_l4_match: {e}")
        return None
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()


def call_llm_for_selection(prompt):
    """Helper function to call LLM and extract ID from response"""
    if LLM_PROVIDER == 'silicon_flow' and not SILICON_FLOW_API_KEY:
        print("[ERROR] API Key æœªé…ç½®ï¼")
        return None
    
    headers = {"Content-Type": "application/json"}
    if LLM_API_KEY:
        headers["Authorization"] = f"Bearer {LLM_API_KEY}"
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False
    }
    
    if LLM_PROVIDER == 'silicon_flow':
        payload["max_tokens"] = 50
        payload["temperature"] = 0.3
    
    try:
        print(f"[LLM] è°ƒç”¨æ¨¡å‹: {LLM_MODEL} (Provider: {LLM_PROVIDER})")
        print(f"[LLM] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        response = requests.post(LLM_API_URL, headers=headers, 
                                data=json.dumps(payload), timeout=60)
        
        print(f"[LLM] å“åº”çŠ¶æ€ç : {response.status_code}")
        
        result = response.json()
        
        if response.status_code != 200:
            print(f"[ERROR] API è¿”å›é”™è¯¯: {result}")
            return None
        
        # å…¼å®¹ä¸åŒæ ¼å¼çš„å“åº”
        if 'choices' in result:
            content = result['choices'][0]['message']['content'].strip()
        elif 'message' in result:
            content = result['message']['content'].strip()
        else:
            print(f"[ERROR] æ— æ³•è§£æå“åº”æ ¼å¼: {result}")
            return None
        
        print(f"[LLM] è¿”å›å†…å®¹: '{content}'")
        
        import re
        match = re.search(r'\d+', content)
        if match:
            selected_id = int(match.group())
            print(f"[LLM] æå–çš„ ID: {selected_id}")
            return selected_id
        else:
            print(f"[ERROR] æ— æ³•ä»è¿”å›å†…å®¹ä¸­æå–æ•°å­— ID")
            return None
            
    except Exception as e:
        print(f"[ERROR] LLM è°ƒç”¨å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    
    return None


def get_l4_info(l4_id):
    """Retrieve basic info for a specific L4 ID from knowledge_base"""
    conn = None
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT l4.name, l3.name, l2.name, l1.name
            FROM knowledge_base l4
            JOIN knowledge_base l3 ON l4.parent_id = l3.id
            JOIN knowledge_base l2 ON l3.parent_id = l2.id
            JOIN knowledge_base l1 ON l2.parent_id = l1.id
            WHERE l4.id = %s AND l4.level = 4
        """, (l4_id,))
        
        result = cursor.fetchone()
        if result:
            return {
                'l4_name': result[0],
                'l3_name': result[1],
                'l2_name': result[2],
                'l1_name': result[3]
            }
        return None
        
    except Exception as e:
        print(f"Error in get_l4_info: {e}")
        return None
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()


def generate_stream_response(user_query, session_id='default', bazi_data=None, user_state=None):
    """Generate streaming response with L4 knowledge boundary and conversation context"""
    
    import sys
    print(f"\n{'='*60}", flush=True)
    print(f"[STREAM] å¼€å§‹ç”Ÿæˆæµå¼å“åº”", flush=True)
    print(f"[STREAM] Session ID: '{session_id}'", flush=True)
    print(f"[STREAM] ç”¨æˆ·é—®é¢˜: '{user_query}'", flush=True)
    print(f"[STREAM] å…«å­—æ•°æ®: {bazi_data}", flush=True)
    print(f"[STREAM] ç”¨æˆ·æ‰€åœ¨å·: {user_state if user_state else 'æœªæŒ‡å®š'}", flush=True)
    print(f"{'='*60}\n", flush=True)
    sys.stdout.flush()
    
    # è·å–ä¼šè¯
    session = get_or_create_session(session_id)
    
    # === V2 æ–°å¢ï¼šè°ƒç”¨ MCP è·å–æ’ç›˜ç»“æœï¼ˆä¼˜åŒ–ï¼šä¼šè¯ä¸­å·²æœ‰åˆ™å¤ç”¨ï¼Œä¸é‡å¤è°ƒç”¨ï¼‰ ===
    bazi_result = None
    bazi_text = session.get('bazi_text')  # å…ˆå°è¯•ä»ä¼šè¯ä¸­è·å–
    
    # åªæœ‰å½“ä¼šè¯ä¸­æ²¡æœ‰å…«å­—ä¿¡æ¯ï¼Œä¸”å‰ç«¯ä¼ äº†æ–°çš„å…«å­—æ•°æ®æ—¶ï¼Œæ‰è°ƒç”¨MCP
    if not bazi_text and bazi_data:
        from .bazi_mcp_client import call_bazi_mcp, format_bazi_for_llm
        
        yield f"data: {json.dumps({'status': 'Getting Bazi chart...'})}\n\n"
        
        print("[MCP] ä¼šè¯ä¸­æ— å…«å­—ä¿¡æ¯ï¼Œå¼€å§‹è°ƒç”¨ bazi-mcp å·¥å…·...", flush=True)
        sys.stdout.flush()
        
        bazi_result = call_bazi_mcp(
            solar_datetime=bazi_data.get('solar_datetime'),
            gender=bazi_data.get('gender', 1)
        )
        
        if bazi_result:
            print("[MCP] âœ… æˆåŠŸè·å–å…«å­—æ’ç›˜ç»“æœï¼Œå·²ä¿å­˜åˆ°ä¼šè¯", flush=True)
            sys.stdout.flush()
            bazi_text = format_bazi_for_llm(bazi_result)
            # ä¿å­˜åˆ°ä¼šè¯ä¸­ï¼Œåç»­å¯¹è¯å¯ä»¥å¤ç”¨
            session['bazi_result'] = bazi_result
            session['bazi_text'] = bazi_text
        else:
            print("[MCP] âŒ è·å–å…«å­—æ’ç›˜å¤±è´¥", flush=True)
            sys.stdout.flush()
    elif bazi_text:
        print("[MCP] âœ… å¤ç”¨ä¼šè¯ä¸­å·²ä¿å­˜çš„å…«å­—ä¿¡æ¯ï¼Œè·³è¿‡MCPè°ƒç”¨", flush=True)
        sys.stdout.flush()
    
    # Send initial status
    yield f"data: {json.dumps({'status': 'Analyzing your question...'})}\n\n"
    
    # æ¯è½®å¯¹è¯éƒ½é‡æ–°åŒ¹é… L4ï¼Œç¡®ä¿ç²¾å‡†å“åº”
    print("[STREAM] è°ƒç”¨ find_best_l4_match...", flush=True)
    sys.stdout.flush()
    
    l4_id = find_best_l4_match(user_query)
    
    print(f"[STREAM] è¿”å›çš„ L4 ID: {l4_id}", flush=True)
    sys.stdout.flush()
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    add_to_history(session_id, 'user', user_query)
    
    # === å…œåº•é€»è¾‘ï¼šå¦‚æœæ²¡æœ‰åŒ¹é…åˆ° L4ï¼Œç›´æ¥ç”¨é€šç”¨ prompt ===
    if not l4_id:
        print("[STREAM] L4 åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æ¨¡å¼å›ç­”", flush=True)
        sys.stdout.flush()
        
        # å‘é€çŠ¶æ€æç¤º
        yield f"data: {json.dumps({'status': 'Answering your question...'})}\n\n"
        
        # æ„å»ºé€šç”¨ promptï¼ˆä¸ä¾èµ–çŸ¥è¯†åº“ï¼‰
        prompt = build_general_prompt(user_query, session['history'][:-1], bazi_text, user_state)
        
        print(f"[STREAM] ä½¿ç”¨é€šç”¨ Promptï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦", flush=True)
        if bazi_text:
            print(f"[STREAM] å·²æ•´åˆå…«å­—ä¿¡æ¯åˆ° Prompt", flush=True)
        
        # è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
        assistant_response = ""
        for chunk in call_llm_stream(prompt):
            if chunk.startswith("data:"):
                yield chunk
                try:
                    data = json.loads(chunk[6:])
                    if 'content' in data:
                        assistant_response += data['content']
                except:
                    pass
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        if assistant_response:
            add_to_history(session_id, 'assistant', assistant_response)
        
        # å‘é€å®Œæˆä¿¡å·
        yield "data: [DONE]\n\n"
        print("[STREAM] æµå¼å“åº”å®Œæˆï¼ˆé€šç”¨æ¨¡å¼ï¼‰", flush=True)
        sys.stdout.flush()
        return
    
    # === æ­£å¸¸æµç¨‹ï¼šåŒ¹é…åˆ°äº† L4 ===
    # Get L4 basic info as semantic boundary
    l4_info = get_l4_info(l4_id)
    
    if not l4_info:
        print("[STREAM] L4ä¿¡æ¯è·å–å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨æ¨¡å¼å›ç­”", flush=True)
        sys.stdout.flush()
        
        # å‘é€çŠ¶æ€æç¤º
        yield f"data: {json.dumps({'status': 'Answering your question...'})}\n\n"
        
        # æ„å»ºé€šç”¨ prompt
        prompt = build_general_prompt(user_query, session['history'][:-1], bazi_text, user_state)
        
        print(f"[STREAM] ä½¿ç”¨é€šç”¨ Promptï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦", flush=True)
        if bazi_text:
            print(f"[STREAM] å·²æ•´åˆå…«å­—ä¿¡æ¯åˆ° Prompt", flush=True)
        
        # è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
        assistant_response = ""
        for chunk in call_llm_stream(prompt):
            if chunk.startswith("data:"):
                yield chunk
                try:
                    data = json.loads(chunk[6:])
                    if 'content' in data:
                        assistant_response += data['content']
                except:
                    pass
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        if assistant_response:
            add_to_history(session_id, 'assistant', assistant_response)
        
        # å‘é€å®Œæˆä¿¡å·
        yield "data: [DONE]\n\n"
        print("[STREAM] æµå¼å“åº”å®Œæˆï¼ˆé€šç”¨æ¨¡å¼ - L4ä¿¡æ¯ç¼ºå¤±ï¼‰", flush=True)
        sys.stdout.flush()
        return
    
    # æ›´æ–°ä¼šè¯ä¸­çš„ L4 ä¿¡æ¯
    session['l4_id'] = l4_id
    session['l4_info'] = l4_info
    
    # Send matched topic
    topic_name = l4_info['l4_name']
    matched_msg = {'status': f'Topic: {topic_name}', 'section': 'header'}
    yield f"data: {json.dumps(matched_msg)}\n\n"
    
    # æ„å»º promptï¼ˆç®€æ´ç‰ˆï¼‰
    prompt = build_contextualized_prompt(user_query, l4_info, session['history'][:-1], bazi_text, user_state)  # å†å²ä¸åŒ…å«å½“å‰é—®é¢˜
    
    print(f"[STREAM] æ„å»ºçŸ¥è¯†åº“å¢å¼º Promptï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦", flush=True)
    if bazi_text:
        print(f"[STREAM] å·²æ•´åˆå…«å­—ä¿¡æ¯åˆ° Prompt", flush=True)
    
    # è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
    assistant_response = ""
    for chunk in call_llm_stream(prompt):
        if chunk.startswith("data:"):
            yield chunk
            # æå–å†…å®¹ç´¯ç§¯ï¼ˆç”¨äºä¿å­˜åˆ°å†å²ï¼‰
            try:
                data = json.loads(chunk[6:])
                if 'content' in data:
                    assistant_response += data['content']
            except:
                pass
    
    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
    if assistant_response:
        add_to_history(session_id, 'assistant', assistant_response)
    
    # Send completion
    yield "data: [DONE]\n\n"
    print(f"[STREAM] æµå¼å“åº”å®Œæˆ", flush=True)
    sys.stdout.flush()


def ask_advisor(request):
    """Handle streaming responses for user questions"""
    if request.method == 'POST':
        user_query = request.POST.get('query', '').strip()
        # ä»è¯·æ±‚ä¸­è·å–æˆ–ç”Ÿæˆ session_id
        session_id = request.POST.get('session_id', '').strip()
        if not session_id:
            import uuid
            session_id = str(uuid.uuid4())
            print(f"[SESSION] ç”Ÿæˆæ–°ä¼šè¯ID: {session_id}")
        else:
            print(f"[SESSION] ä½¿ç”¨ç°æœ‰ä¼šè¯ID: {session_id}")
        
        # === V2 æ–°å¢ï¼šè·å–å…«å­—æ•°æ® ===
        bazi_data_str = request.POST.get('bazi_data', '').strip()
        bazi_data = None
        if bazi_data_str:
            try:
                bazi_data = json.loads(bazi_data_str)
                print(f"[BAZI] æ”¶åˆ°å‘½ç†æ•°æ®: {bazi_data}")
            except json.JSONDecodeError:
                print(f"[BAZI] è§£æå‘½ç†æ•°æ®å¤±è´¥: {bazi_data_str}")
        
        # === V4 æ–°å¢ï¼šè·å–ç”¨æˆ·æ‰€åœ¨å·ï¼ˆæ–‡åŒ–é€‚é…ï¼‰ ===
        user_state = request.POST.get('user_state', '').strip()
        if user_state:
            print(f"[CULTURAL] ç”¨æˆ·æ‰€åœ¨å·: {user_state}")
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"\n{'='*60}")
        print(f"[REQUEST] æ”¶åˆ°ç”¨æˆ·é—®é¢˜: '{user_query}'")
        print(f"[REQUEST] ä¼šè¯ID: {session_id}")
        print(f"[REQUEST] å…«å­—æ•°æ®: {'æœ‰' if bazi_data else 'æ— '}")
        print(f"[REQUEST] ç”¨æˆ·æ‰€åœ¨å·: {user_state if user_state else 'æœªæŒ‡å®š'}")
        print(f"[REQUEST] API Keyå­˜åœ¨: {bool(SILICON_FLOW_API_KEY)}")
        print(f"[REQUEST] ä½¿ç”¨æ¨¡å‹: {LLM_MODEL}")
        print(f"{'='*60}\n")
        
        if not user_query:
            print("[ERROR] ç”¨æˆ·é—®é¢˜ä¸ºç©º")
            return StreamingHttpResponse(
                iter([f"data: {json.dumps({'error': 'Please enter a question'})}\n\n"]),
                content_type='text/event-stream'
            )
        
        response = StreamingHttpResponse(
            generate_stream_response(user_query, session_id, bazi_data, user_state),
            content_type='text/event-stream'
        )
        response['Cache-Control'] = 'no-cache'
        response['X-Session-ID'] = session_id  # é€šè¿‡å“åº”å¤´è¿”å› session_id
        response['X-Accel-Buffering'] = 'no'
        return response
    
    return render(request, 'advisor/index.html')

