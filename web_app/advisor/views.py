import os
import json
import time
import mysql.connector
import requests
from django.shortcuts import render
from django.http import StreamingHttpResponse
from dotenv import load_dotenv

load_dotenv()

# Configuration
SILICON_FLOW_API_URL = 'https://api.siliconflow.cn/v1/chat/completions'
SILICON_FLOW_API_KEY = os.getenv('SILICON_FLOW_API_KEY')

# å»æ‰ç¯å¢ƒå˜é‡ä¸­å¯èƒ½çš„å¼•å·
if SILICON_FLOW_API_KEY:
    SILICON_FLOW_API_KEY = SILICON_FLOW_API_KEY.strip('"').strip("'")

DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost').strip('"').strip("'"),
    'user': os.getenv('DB_USER', 'root').strip('"').strip("'"),
    'password': os.getenv('DB_PASSWORD', '').strip('"').strip("'"),
    'database': os.getenv('DB_NAME', 'mysql').strip('"').strip("'"),
    'pool_name': 'mypool',
    'pool_size': 5,
    'pool_reset_session': True,
    'autocommit': True
}

# ä½¿ç”¨å¿«é€Ÿæ¨¡å‹è¿›è¡Œç®€å•çš„ ID é€‰æ‹©ä»»åŠ¡
LLM_MODEL = "Qwen/Qwen3-32B"  # ä¿®æ­£æ¨¡å‹åç§°
# å¸¸è§çš„å¯ç”¨æ¨¡å‹ï¼š
# - "Qwen/Qwen2.5-7B-Instruct"
# - "deepseek-ai/DeepSeek-V2.5"
# - "01-ai/Yi-1.5-9B-Chat"

# ä¼šè¯ç®¡ç†ï¼šå­˜å‚¨å¤šè½®å¯¹è¯å†å²ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis/æ•°æ®åº“ï¼‰
SESSION_STORE = {}
# ç»“æ„: {session_id: {'history': [{'role': 'user', 'content': '...'}, ...], 'l4_id': int, 'l4_content': dict}}

def get_or_create_session(session_id):
    """è·å–æˆ–åˆ›å»ºä¼šè¯"""
    if session_id not in SESSION_STORE:
        SESSION_STORE[session_id] = {
            'history': [],
            'l4_id': None,
            'l4_content': None
        }
    return SESSION_STORE[session_id]

def add_to_history(session_id, role, content):
    """æ·»åŠ æ¶ˆæ¯åˆ°ä¼šè¯å†å²"""
    session = get_or_create_session(session_id)
    session['history'].append({'role': role, 'content': content})
    # é™åˆ¶å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘10è½®ï¼‰
    if len(session['history']) > 20:  # 10è½®å¯¹è¯ = 20æ¡æ¶ˆæ¯
        session['history'] = session['history'][-20:]


def index(request):
    """Render the main advisor interface"""
    return render(request, 'advisor/index.html')


# åœºæ™¯äººæ ¼æ˜ å°„è¡¨
PERSONA_MAPPING = {
    # æƒ…æ„Ÿä¸äººé™…å…³ç³»
    'dating': 'a bold Fashion Editor meets Energy Healer',
    'relationship': 'a straight-talking Relationship Coach with cosmic insights',
    'breakup': 'a compassionate yet brutally honest Therapist',
    'friendship': 'a wise Life Coach who keeps it real',
    
    # èŒåœºä¸äº‹ä¸š
    'workplace': 'a sharp Corporate Strategist with Zen wisdom',
    'career': 'a visionary Career Mentor who sees the bigger picture',
    'conflict': 'a tough-love Mediator with clarity',
    'leadership': 'a confident Executive Coach',
    
    # ä¸ªäººæˆé•¿
    'decision': 'a decisive Life Strategist',
    'habit': 'a no-nonsense Performance Coach',
    'confidence': 'a fierce Empowerment Coach',
    
    # é»˜è®¤
    'default': 'a bold, intuitive Energy Strategist'
}

# äº”è¡Œèƒ½é‡æ˜ å°„ï¼ˆåŒ—ç¾åŒ–è¡¨è¾¾ï¼‰
ENERGY_ELEMENTS = {
    'wood': 'Growth Energy - bold, expansive, forward-moving',
    'fire': 'Passion Energy - magnetic, expressive, confident',
    'earth': 'Grounding Energy - stable, centering, reliable',
    'metal': 'Clarity Energy - sharp, decisive, structured',
    'water': 'Flow Energy - adaptive, intuitive, resilient'
}

def get_persona_for_l4(l4_name):
    """æ ¹æ® L4 åœºæ™¯åŠ¨æ€é€‰æ‹© AI äººæ ¼"""
    l4_lower = l4_name.lower()
    
    # åŒ¹é…å…³é”®è¯
    for keyword, persona in PERSONA_MAPPING.items():
        if keyword in l4_lower:
            return persona
    
    return PERSONA_MAPPING['default']

def build_contextualized_prompt(user_query, l4_info, conversation_history):
    """æ„å»ºåŒ…å« L4 è¯­ä¹‰è¾¹ç•Œå’Œå¯¹è¯å†å²çš„ prompt"""
    
    # åŠ¨æ€é€‰æ‹©äººæ ¼
    persona = get_persona_for_l4(l4_info['l4_name'])
    
    # ç³»ç»Ÿè§’è‰²å®šä¹‰ - V2 çŠ€åˆ©ç‰ˆ
    system_role = f"""You are {persona}.

Your Style:
- Be DIRECT and CONFIDENT. No "you could try" or "it might be good to" - say "Do this" or "Don't do that".
- Give ONE clear instruction, not 10 vague suggestions.
- Be the friend who tells the truth, not the one who says "both options are fine".
- Use natural, conversational English with personality.

Output Structure (MANDATORY):
1. **The Move:** (1-2 sentences, imperative mood) - What to do RIGHT NOW
2. **Why It Works:** (2-3 sentences max) - The energy/logic behind it
3. **Your Mantra:** (1 power phrase) - A quote to own this decision

Example:
**The Move:** Wear the beige trench coat with gold accessories.
**Why It Works:** You're carrying too much emotional water todayâ€”beige grounds that energy like earth absorbing a flood. Gold adds a boundary, a signal that says "I'm here, but I'm not scattered."
**Your Mantra:** "I am not asking for space. I am claiming it."

Rules:
- Stay within the topic scope
- Keep total response under 100 words
- Reference energy qualities naturally (Growth, Passion, Grounding, Clarity, Flow) - never say "Wood/Fire/Earth/Metal/Water"
- Be bold but not rude. Think: confident best friend."""
    
    # çŸ¥è¯†è¾¹ç•Œï¼ˆL4 ä¸»é¢˜ä½œä¸ºè¯­ä¹‰è¾¹ç•Œï¼‰
    knowledge_boundary = f"""

ğŸ“ Topic Focus: {l4_info['l4_name']}
   (Context: {l4_info['l1_name']} â†’ {l4_info['l2_name']} â†’ {l4_info['l3_name']})

âš¡ Energy Toolbox (use naturally, don't explain):
   â€¢ Growth Energy - bold, expansive, forward-moving
   â€¢ Passion Energy - magnetic, expressive, confident  
   â€¢ Grounding Energy - stable, centering, reliable
   â€¢ Clarity Energy - sharp, decisive, structured
   â€¢ Flow Energy - adaptive, intuitive, resilient
"""

    # å¯¹è¯å†å²ï¼ˆæœ€è¿‘10è½®ï¼‰
    history_text = ""
    if conversation_history:
        recent_history = conversation_history[-20:]  # æœ€è¿‘10è½®ï¼ˆæ¯è½®2æ¡æ¶ˆæ¯ï¼‰
        history_text = "\n\nConversation Context:\n"
        for msg in recent_history:
            role_label = "User" if msg['role'] == 'user' else "You"
            history_text += f"{role_label}: {msg['content']}\n"
    
    # å½“å‰é—®é¢˜
    current_question = f"""\n\nğŸ’¬ User Question: "{user_query}"

ğŸ¯ Now respond following the 3-part structure:
   **The Move:** [Direct instruction]
   **Why It Works:** [Brief energy/logic explanation]
   **Your Mantra:** [Power quote]

Keep it under 100 words total. Be direct, be confident, be actionable."""
    
    # ç»„åˆå®Œæ•´ prompt
    full_prompt = system_role + knowledge_boundary + history_text + current_question
    
    return full_prompt

def generate_decision_header(user_query, l4_info):
    """
    ç”Ÿæˆå†³ç­–å¤´éƒ¨ï¼šä¿¡å·ç¯ + èƒ½é‡ç±»å‹ + æ ¸å¿ƒæŒ‡ä»¤
    ä½¿ç”¨å¿«é€Ÿ LLM è°ƒç”¨ï¼ˆéæµå¼ï¼‰
    """
    if not SILICON_FLOW_API_KEY:
        return None
    
    prompt = f"""Based on this question: "{user_query}"
Topic: {l4_info['l4_name']}

Generate a quick decision header in JSON format:
{{
  "signal": "ğŸŸ¢" or "ğŸŸ¡" or "ğŸ”´",
  "vibe": "one of: Growth Energy / Passion Energy / Grounding Energy / Clarity Energy / Flow Energy",
  "instruction": "one short imperative sentence (5-8 words)"
}}

Rules:
- ğŸŸ¢ Green = Go for it, confident move
- ğŸŸ¡ Yellow = Proceed with caution
- ğŸ”´ Red = Stop, reconsider
- Choose the energy that fits best
- Instruction must be direct and actionable

Respond ONLY with valid JSON, no explanation."""

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {SILICON_FLOW_API_KEY}"
    }
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 150,
        "temperature": 0.5
    }
    
    try:
        response = requests.post(SILICON_FLOW_API_URL, headers=headers, 
                                data=json.dumps(payload), timeout=30)
        result = response.json()
        content = result['choices'][0]['message']['content'].strip()
        
        # å°è¯•è§£æ JSON
        import re
        json_match = re.search(r'\{[^}]+\}', content, re.DOTALL)
        if json_match:
            decision = json.loads(json_match.group())
            return decision
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        return {
            "signal": "ğŸŸ¢",
            "vibe": "Clarity Energy",
            "instruction": "Trust your instinct and move forward"
        }
    except Exception as e:
        print(f"[ERROR] ç”Ÿæˆå†³ç­–å¤´éƒ¨å¤±è´¥: {e}")
        return None

def call_llm_stream(prompt: str):
    """
    Call LLM API with streaming enabled.
    Yields chunks of text as they arrive.
    """
    if not SILICON_FLOW_API_KEY:
        yield "data: Error: API key not configured\n\n"
        return

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {SILICON_FLOW_API_KEY}"
    }
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 2048,
        "temperature": 0.7,
        "stream": True  # Enable streaming
    }

    try:
        response = requests.post(
            SILICON_FLOW_API_URL, 
            headers=headers, 
            data=json.dumps(payload), 
            stream=True,
            timeout=120
        )
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                line_text = line.decode('utf-8')
                if line_text.startswith('data: '):
                    line_text = line_text[6:]  # Remove 'data: ' prefix
                    
                    if line_text.strip() == '[DONE]':
                        break
                        
                    try:
                        data = json.loads(line_text)
                        if 'choices' in data and len(data['choices']) > 0:
                            delta = data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                # Send as SSE format
                                yield f"data: {json.dumps({'content': content})}\n\n"
                    except json.JSONDecodeError:
                        continue
                        
    except Exception as e:
        yield f"data: {json.dumps({'error': str(e)})}\n\n"


def find_best_l4_match(user_query):
    """Find the best matching L4 intention for the user query with hierarchical search"""
    conn = None
    try:
        print(f"[MATCH] å¼€å§‹åŒ¹é…æµç¨‹ï¼Œç”¨æˆ·é—®é¢˜: '{user_query}'")
        
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # Step 1: Find best matching L1 Domain
        print("[MATCH] æ­¥éª¤ 1/4: æŸ¥è¯¢æ‰€æœ‰ L1 é¢†åŸŸ...")
        cursor.execute("SELECT id, name, description_en FROM knowledge_base WHERE level = 1")
        l1_candidates = cursor.fetchall()
        
        print(f"[MATCH] æ‰¾åˆ° {len(l1_candidates)} ä¸ª L1 é¢†åŸŸ")
        
        if not l1_candidates:
            print("[ERROR] æ•°æ®åº“ä¸­æ²¡æœ‰ L1 æ•°æ®ï¼")
            return None
        
        # Use LLM to select best L1
        l1_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:100] if c[2] else ''}" for c in l1_candidates])
        l1_prompt = f"""User Query: "{user_query}"

Available Life Domains (L1):
{l1_list}

Task: Select the single most relevant Life Domain ID that best matches the user's question.
Return ONLY the ID number."""
        
        print("[MATCH] è°ƒç”¨ LLM é€‰æ‹© L1...")
        best_l1_id = call_llm_for_selection(l1_prompt)
        if not best_l1_id:
            print("[ERROR] L1 åŒ¹é…å¤±è´¥ï¼ŒLLM æœªè¿”å›æœ‰æ•ˆ ID")
            return None
        
        print(f"[Match] L1 Domain ID: {best_l1_id}")
        
        # Step 2: Find best matching L2 Scenario under the selected L1
        cursor.execute("""
            SELECT id, name, description_en 
            FROM knowledge_base 
            WHERE level = 2 AND parent_id = %s
        """, (best_l1_id,))
        l2_candidates = cursor.fetchall()
        
        if not l2_candidates:
            return None
        
        l2_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:100] if c[2] else ''}" for c in l2_candidates])
        l2_prompt = f"""User Query: "{user_query}"

Available Scenarios (L2):
{l2_list}

Task: Select the single most relevant Scenario ID that best matches the user's specific situation.
Return ONLY the ID number."""
        
        best_l2_id = call_llm_for_selection(l2_prompt)
        if not best_l2_id:
            return None
        
        print(f"[Match] L2 Scenario ID: {best_l2_id}")
        
        # Step 3: Find best matching L3 Sub-scenario under the selected L2
        cursor.execute("""
            SELECT id, name, description_en 
            FROM knowledge_base 
            WHERE level = 3 AND parent_id = %s
        """, (best_l2_id,))
        l3_candidates = cursor.fetchall()
        
        if not l3_candidates:
            return None
        
        l3_list = "\n".join([f"ID {c[0]}: {c[1]} - {c[2][:80] if c[2] else ''}" for c in l3_candidates])
        l3_prompt = f"""User Query: "{user_query}"

Available Sub-scenarios (L3):
{l3_list}

Task: Select the single most relevant Sub-scenario ID.
Return ONLY the ID number."""
        
        best_l3_id = call_llm_for_selection(l3_prompt)
        if not best_l3_id:
            return None
        
        print(f"[Match] L3 Sub-scenario ID: {best_l3_id}")
        
        # Step 4: Find best matching L4 Intention under the selected L3
        cursor.execute("""
            SELECT kb.id, kb.name, kb.description_en 
            FROM knowledge_base kb
            JOIN l4_content c ON kb.id = c.l4_id
            WHERE kb.level = 4 AND kb.parent_id = %s
        """, (best_l3_id,))
        l4_candidates = cursor.fetchall()
        
        if not l4_candidates:
            # Fallback: try to find any L4 with content under this L3
            cursor.execute("""
                SELECT kb.id, kb.name, kb.description_en 
                FROM knowledge_base kb
                WHERE kb.level = 4 AND kb.parent_id = %s
                LIMIT 1
            """, (best_l3_id,))
            fallback = cursor.fetchone()
            if fallback:
                print(f"[Match] L4 Intention ID (fallback): {fallback[0]}")
                return fallback[0]
            return None
        
        l4_list = "\n".join([f"ID {c[0]}: {c[1]}" for c in l4_candidates])
        l4_prompt = f"""User Query: "{user_query}"

Available User Intentions (L4):
{l4_list}

Task: Select the single most relevant Intention ID that exactly matches what the user wants to know.
Return ONLY the ID number."""
        
        best_l4_id = call_llm_for_selection(l4_prompt)
        print(f"[Match] L4 Intention ID: {best_l4_id}")
        
        return best_l4_id
        
    except Exception as e:
        print(f"Error in find_best_l4_match: {e}")
        return None
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()


def call_llm_for_selection(prompt):
    """Helper function to call LLM and extract ID from response"""
    if not SILICON_FLOW_API_KEY:
        print("[ERROR] API Key æœªé…ç½®ï¼")
        return None
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {SILICON_FLOW_API_KEY}"
    }
    
    payload = {
        "model": LLM_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 50,
        "temperature": 0.3
    }
    
    try:
        print(f"[LLM] è°ƒç”¨æ¨¡å‹: {LLM_MODEL}")
        print(f"[LLM] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        response = requests.post(SILICON_FLOW_API_URL, headers=headers, 
                                data=json.dumps(payload), timeout=60)
        
        print(f"[LLM] å“åº”çŠ¶æ€ç : {response.status_code}")
        
        result = response.json()
        
        # å¦‚æœæ˜¯é”™è¯¯å“åº”ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        if response.status_code != 200:
            print(f"[ERROR] API è¿”å›é”™è¯¯: {result}")
            return None
        
        content = result['choices'][0]['message']['content'].strip()
        
        print(f"[LLM] è¿”å›å†…å®¹: '{content}'")
        
        import re
        match = re.search(r'\d+', content)
        if match:
            selected_id = int(match.group())
            print(f"[LLM] æå–çš„ ID: {selected_id}")
            return selected_id
        else:
            print(f"[ERROR] æ— æ³•ä»è¿”å›å†…å®¹ä¸­æå–æ•°å­— ID")
            return None
            
    except Exception as e:
        print(f"[ERROR] LLM è°ƒç”¨å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    
    return None


def get_l4_info(l4_id):
    """Retrieve basic info for a specific L4 ID from knowledge_base"""
    conn = None
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT l4.name, l3.name, l2.name, l1.name
            FROM knowledge_base l4
            JOIN knowledge_base l3 ON l4.parent_id = l3.id
            JOIN knowledge_base l2 ON l3.parent_id = l2.id
            JOIN knowledge_base l1 ON l2.parent_id = l1.id
            WHERE l4.id = %s AND l4.level = 4
        """, (l4_id,))
        
        result = cursor.fetchone()
        if result:
            return {
                'l4_name': result[0],
                'l3_name': result[1],
                'l2_name': result[2],
                'l1_name': result[3]
            }
        return None
        
    except Exception as e:
        print(f"Error in get_l4_info: {e}")
        return None
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()


def generate_stream_response(user_query, session_id='default'):
    """Generate streaming response with L4 knowledge boundary and conversation context"""
    
    import sys
    print(f"\n{'='*60}", flush=True)
    print(f"[STREAM] å¼€å§‹ç”Ÿæˆæµå¼å“åº”", flush=True)
    print(f"[STREAM] Session ID: '{session_id}'", flush=True)
    print(f"[STREAM] ç”¨æˆ·é—®é¢˜: '{user_query}'", flush=True)
    print(f"{'='*60}\n", flush=True)
    sys.stdout.flush()
    
    # è·å–ä¼šè¯
    session = get_or_create_session(session_id)
    
    # Send initial status
    yield f"data: {json.dumps({'status': 'Analyzing your question...'})}\n\n"
    
    # æ¯è½®å¯¹è¯éƒ½é‡æ–°åŒ¹é… L4ï¼Œç¡®ä¿ç²¾å‡†å“åº”
    print("[STREAM] è°ƒç”¨ find_best_l4_match...", flush=True)
    sys.stdout.flush()
    
    l4_id = find_best_l4_match(user_query)
    
    print(f"[STREAM] è¿”å›çš„ L4 ID: {l4_id}", flush=True)
    sys.stdout.flush()
    
    if not l4_id:
        print("[STREAM] L4 ID ä¸º Noneï¼Œè¿”å›é”™è¯¯", flush=True)
        sys.stdout.flush()
        yield f"data: {json.dumps({'error': 'Could not find a relevant topic in our knowledge base.'})}\n\n"
        yield "data: [DONE]\n\n"
        print("[STREAM] æµå¼å“åº”å®Œæˆï¼ˆé”™è¯¯é€€å‡ºï¼‰", flush=True)
        return
    
    # Get L4 basic info as semantic boundary
    l4_info = get_l4_info(l4_id)
    
    if not l4_info:
        yield f"data: {json.dumps({'error': 'Topic not found in knowledge base.'})}\n\n"
        yield "data: [DONE]\n\n"
        print("[STREAM] æµå¼å“åº”å®Œæˆï¼ˆL4ä¿¡æ¯æœªæ‰¾åˆ°ï¼‰", flush=True)
        return
    
    # æ›´æ–°ä¼šè¯ä¸­çš„ L4 ä¿¡æ¯
    session['l4_id'] = l4_id
    session['l4_info'] = l4_info
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    add_to_history(session_id, 'user', user_query)
    
    # Send matched topic
    topic_name = l4_info['l4_name']
    matched_msg = {'status': f'Topic: {topic_name}', 'section': 'header'}
    yield f"data: {json.dumps(matched_msg)}\n\n"
    
    # === V2 æ–°å¢ï¼šç”Ÿæˆå†³ç­–å¤´éƒ¨ï¼ˆçº¢ç»¿ç¯ç³»ç»Ÿï¼‰ ===
    print("[STREAM] ç”Ÿæˆå†³ç­–å¤´éƒ¨...", flush=True)
    decision_header = generate_decision_header(user_query, l4_info)
    if decision_header:
        yield f"data: {json.dumps({'decision_header': decision_header})}\n\n"
        print(f"[STREAM] å†³ç­–å¤´éƒ¨: {decision_header}", flush=True)
    
    # æ„å»º promptï¼ˆåŒ…å« L4 è¯­ä¹‰è¾¹ç•Œ + å¯¹è¯å†å²ï¼‰
    prompt = build_contextualized_prompt(user_query, l4_info, session['history'][:-1])  # å†å²ä¸åŒ…å«å½“å‰é—®é¢˜
    
    print(f"[STREAM] æ„å»ºçš„ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦", flush=True)
    
    # è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
    assistant_response = ""
    for chunk in call_llm_stream(prompt):
        if chunk.startswith("data:"):
            yield chunk
            # æå–å†…å®¹ç´¯ç§¯ï¼ˆç”¨äºä¿å­˜åˆ°å†å²ï¼‰
            try:
                data = json.loads(chunk[6:])
                if 'content' in data:
                    assistant_response += data['content']
            except:
                pass
    
    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
    if assistant_response:
        add_to_history(session_id, 'assistant', assistant_response)
    
    # Send completion
    yield "data: [DONE]\n\n"
    print(f"[STREAM] æµå¼å“åº”å®Œæˆ", flush=True)
    sys.stdout.flush()


def ask_advisor(request):
    """Handle streaming responses for user questions"""
    if request.method == 'POST':
        user_query = request.POST.get('query', '').strip()
        # ä»è¯·æ±‚ä¸­è·å–æˆ–ç”Ÿæˆ session_id
        session_id = request.POST.get('session_id', '').strip()
        if not session_id:
            import uuid
            session_id = str(uuid.uuid4())
            print(f"[SESSION] ç”Ÿæˆæ–°ä¼šè¯ID: {session_id}")
        else:
            print(f"[SESSION] ä½¿ç”¨ç°æœ‰ä¼šè¯ID: {session_id}")
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"\n{'='*60}")
        print(f"[REQUEST] æ”¶åˆ°ç”¨æˆ·é—®é¢˜: '{user_query}'")
        print(f"[REQUEST] ä¼šè¯ID: {session_id}")
        print(f"[REQUEST] API Keyå­˜åœ¨: {bool(SILICON_FLOW_API_KEY)}")
        print(f"[REQUEST] ä½¿ç”¨æ¨¡å‹: {LLM_MODEL}")
        print(f"{'='*60}\n")
        
        if not user_query:
            print("[ERROR] ç”¨æˆ·é—®é¢˜ä¸ºç©º")
            return StreamingHttpResponse(
                iter([f"data: {json.dumps({'error': 'Please enter a question'})}\n\n"]),
                content_type='text/event-stream'
            )
        
        response = StreamingHttpResponse(
            generate_stream_response(user_query, session_id),
            content_type='text/event-stream'
        )
        response['Cache-Control'] = 'no-cache'
        response['X-Session-ID'] = session_id  # é€šè¿‡å“åº”å¤´è¿”å› session_id
        response['X-Accel-Buffering'] = 'no'
        return response
    
    return render(request, 'advisor/index.html')

